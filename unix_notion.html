<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>unix_pipeline_walkthrough</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
/* Minimal sensible defaults for Notion paste */
:root { color-scheme: light; }
body { font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Arial, "Apple Color Emoji", "Segoe UI Emoji"; line-height:1.6; font-size:16px; }
h1,h2,h3,h4 { line-height:1.25; margin:1.2em 0 .5em; }
p,ul,ol,pre,figure,table,blockquote { margin: .7em 0; }
code,kbd,samp { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "DejaVu Sans Mono", Consolas, "Liberation Mono", "Courier New", monospace; }
figure.code-image { display:block; margin: .75em 0; }
figure.code-image > img { max-width: 100%; height: auto; display: block; }
blockquote { padding-left: .9em; border-left: 3px solid #e5e7eb; color:#374151; }
hr { border:0; border-top:1px solid #e5e7eb; margin:1.5em 0; }
table { border-collapse: collapse; width: 100%; }
th, td { border: 1px solid #e5e7eb; padding: .4em .6em; text-align: left; }
a { color: #2563eb; text-decoration: none; }
a:hover { text-decoration: underline; }
</style>
  </head>
  <body>
<h1>Unix Pipeline and CLI Tools Walkthrough</h1>
<p>This walkthrough demonstrates the Unix philosophy of building data processing pipelines using small, composable command-line tools. You'll learn to chain together tools like <code>curl</code>, <code>jq</code>, <code>pup</code>, <code>rg</code>, and <code>sqlite-utils</code> to process data efficiently.</p>
<h2>Prerequisites</h2>
<p>Install required command-line tools:</p>
<pre><code class="language-bash"># Install uv (Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install sqlite-utils (database toolkit)
uvx install sqlite-utils

# Install pup (HTML parser)
brew install pup

# Install jq (JSON processor) - usually pre-installed
which jq
</code></pre>
<h2>The Unix Philosophy</h2>
<p>Unix tools follow these principles:</p>
<ol>
<li><strong>Do one thing well</strong>: Each tool has a single, focused purpose</li>
<li><strong>Work together</strong>: Tools can be chained via pipes (<code>|</code>)</li>
<li><strong>Handle text streams</strong>: Tools read from stdin, write to stdout</li>
<li><strong>Be composable</strong>: Complex tasks built from simple components</li>
</ol>
<h2>Step 1: Download the Processing Script</h2>
<p>Get the HTML menu extraction script:</p>
<pre><code class="language-bash">curl -o extract_menu_unix.py https://raw.githubusercontent.com/mpr1255/2025s1_mlci/master/week2--scraping_api_cli/mensa_scraping/second_example/extract_menu_unix.py
</code></pre>
<h2>Step 2: Understanding the Script Structure</h2>
<p>The script follows Unix conventions:</p>
<pre><code class="language-python">#!/usr/bin/env uv run --quiet --script
# /// script
# dependencies = [&quot;beautifulsoup4&quot;]
# ///
</code></pre>
<p>Key features:</p>
<ul>
<li><strong>Shebang</strong>: Tells system to run with <code>uv</code></li>
<li><strong>Inline dependencies</strong>: No separate requirements file needed</li>
<li><strong>Stdin/stdout</strong>: Reads file paths from stdin, outputs JSONL to stdout</li>
<li><strong>JSONL format</strong>: One JSON object per line (stream-friendly)</li>
</ul>
<h2>Step 3: Basic Pipeline Operations</h2>
<h3>Single File Processing</h3>
<p>Process one HTML file:</p>
<pre><code class="language-bash">echo &quot;path/to/menu.html&quot; | python extract_menu_unix.py
</code></pre>
<h3>Multiple Files with Find</h3>
<p>Find and process all HTML files:</p>
<pre><code class="language-bash">find data_wget -name &quot;*.html&quot; | python extract_menu_unix.py
</code></pre>
<h3>Filter with Grep/Ripgrep</h3>
<p>Find files containing specific content, then process:</p>
<pre><code class="language-bash"># Using ripgrep (faster)
rg -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py

# Using traditional grep
grep -r -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py
</code></pre>
<h2>Step 4: JSON Processing with jq</h2>
<h3>Pretty Print JSON</h3>
<p>Format output for human reading:</p>
<pre><code class="language-bash">rg -l &quot;aw-weekly-menu&quot; data_wget | head -1 | python extract_menu_unix.py | jq .
</code></pre>
<h3>Collect Stream to Array</h3>
<p>Convert JSONL (stream) to JSON array:</p>
<pre><code class="language-bash">rg -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py | jq -s '.'
</code></pre>
<h3>Filter and Transform</h3>
<p>Extract specific fields:</p>
<pre><code class="language-bash"># Get only descriptions and prices
rg -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py | jq '{description, price_students}'

# Filter by city
rg -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py | jq 'select(.city == &quot;mainz&quot;)'
</code></pre>
<h2>Step 5: Database Integration with sqlite-utils</h2>
<h3>Direct Insert from Pipeline</h3>
<p>Insert JSONL stream directly into SQLite:</p>
<pre><code class="language-bash">rg -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py | sqlite-utils insert menu.db meals -
</code></pre>
<h3>Insert JSON Array</h3>
<p>Convert to array first, then insert:</p>
<pre><code class="language-bash">rg -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py | jq -s '.' | sqlite-utils insert menu.db meals -
</code></pre>
<h3>Query the Database</h3>
<pre><code class="language-bash"># View table schema
sqlite-utils schema menu.db meals

# Query data
sqlite-utils query menu.db &quot;SELECT city, COUNT(*) as meal_count FROM meals GROUP BY city&quot;

# Export to CSV
sqlite-utils query menu.db &quot;SELECT * FROM meals WHERE price_students &lt; 5.0&quot; --csv &gt; cheap_meals.csv
</code></pre>
<h2>Step 6: Building Complex Pipelines</h2>
<h3>Multi-step Processing</h3>
<pre><code class="language-bash"># 1. Find files → 2. Extract data → 3. Filter → 4. Store
rg -l &quot;aw-weekly-menu&quot; data_wget \
  | python extract_menu_unix.py \
  | jq 'select(.price_students != null and .price_students &lt; 10.0)' \
  | sqlite-utils insert menu.db affordable_meals -
</code></pre>
<h3>Error Handling and Logging</h3>
<pre><code class="language-bash"># Capture errors separately
rg -l &quot;aw-weekly-menu&quot; data_wget \
  | python extract_menu_unix.py 2&gt;errors.log \
  | sqlite-utils insert menu.db meals - \
  &amp;&amp; echo &quot;Success: $(wc -l &lt; errors.log) errors logged&quot;
</code></pre>
<h3>Parallel Processing</h3>
<pre><code class="language-bash"># Process files in parallel (if you have GNU parallel)
rg -l &quot;aw-weekly-menu&quot; data_wget | parallel -j4 &quot;echo {} | python extract_menu_unix.py&quot;
</code></pre>
<h2>Step 7: Data Validation and Quality Control</h2>
<h3>Count Records at Each Step</h3>
<pre><code class="language-bash"># Count files found
echo &quot;Files found: $(rg -l 'aw-weekly-menu' data_wget | wc -l)&quot;

# Count records extracted
echo &quot;Records extracted: $(rg -l 'aw-weekly-menu' data_wget | python extract_menu_unix.py | wc -l)&quot;

# Count records in database
echo &quot;Records in DB: $(sqlite-utils query menu.db 'SELECT COUNT(*) FROM meals' --raw-output)&quot;
</code></pre>
<h3>Data Quality Checks</h3>
<pre><code class="language-bash"># Check for missing prices
rg -l &quot;aw-weekly-menu&quot; data_wget \
  | python extract_menu_unix.py \
  | jq 'select(.price_students == null)' \
  | wc -l

# Find duplicate meal_ids
sqlite-utils query menu.db &quot;SELECT meal_id, COUNT(*) as count FROM meals GROUP BY meal_id HAVING count &gt; 1&quot;
</code></pre>
<h2>Step 8: Advanced Pipeline Patterns</h2>
<h3>Conditional Processing</h3>
<pre><code class="language-bash"># Process only if output file doesn't exist
[ ! -f menu.db ] &amp;&amp; {
  rg -l &quot;aw-weekly-menu&quot; data_wget \
    | python extract_menu_unix.py \
    | sqlite-utils insert menu.db meals -
}
</code></pre>
<h3>Incremental Updates</h3>
<pre><code class="language-bash"># Find new files (modified in last day)
find data_wget -name &quot;*.html&quot; -mtime -1 \
  | python extract_menu_unix.py \
  | sqlite-utils insert menu.db meals - --replace
</code></pre>
<h3>Data Transformation Pipeline</h3>
<pre><code class="language-bash"># Complex transformation: normalize prices, add metadata
rg -l &quot;aw-weekly-menu&quot; data_wget \
  | python extract_menu_unix.py \
  | jq '. + {
      price_normalized: (.price_students // 0),
      processed_at: now | strftime(&quot;%Y-%m-%d %H:%M:%S&quot;),
      is_affordable: (.price_students // 999) &lt; 5.0
    }' \
  | sqlite-utils insert menu.db enriched_meals -
</code></pre>
<h2>Step 9: Testing and Debugging</h2>
<h3>Test with Limited Data</h3>
<pre><code class="language-bash"># Process only first file
rg -l &quot;aw-weekly-menu&quot; data_wget | head -1 | python extract_menu_unix.py | jq .

# Process first 3 files
rg -l &quot;aw-weekly-menu&quot; data_wget | head -3 | python extract_menu_unix.py | jq -s '.'
</code></pre>
<h3>Verbose Debugging</h3>
<pre><code class="language-bash"># Add debugging at each step
rg -l &quot;aw-weekly-menu&quot; data_wget | tee found_files.txt \
  | python extract_menu_unix.py | tee extracted_data.jsonl \
  | jq -s '.' | tee final_array.json \
  | sqlite-utils insert menu.db meals -
</code></pre>
<h3>Performance Monitoring</h3>
<pre><code class="language-bash"># Time the pipeline
time (rg -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py | sqlite-utils insert menu.db meals -)

# Monitor memory usage
/usr/bin/time -v rg -l &quot;aw-weekly-menu&quot; data_wget | python extract_menu_unix.py | sqlite-utils insert menu.db meals -
</code></pre>
<h2>Key Learning Points</h2>
<ul>
<li><strong>Stream processing</strong>: Unix pipes enable processing large datasets without loading everything into memory</li>
<li><strong>Tool composition</strong>: Complex tasks built from simple, focused tools</li>
<li><strong>JSONL format</strong>: Stream-friendly alternative to JSON arrays</li>
<li><strong>Error handling</strong>: Use stderr for errors, stdout for data</li>
<li><strong>Incremental processing</strong>: Build pipelines that can handle updates efficiently</li>
<li><strong>Testing</strong>: Always test with small datasets first</li>
<li><strong>Debugging</strong>: Use <code>tee</code> to capture intermediate results</li>
</ul>
<h2>Best Practices</h2>
<ol>
<li><strong>Start small</strong>: Test each pipeline step individually</li>
<li><strong>Use standards</strong>: Follow JSON/JSONL conventions for data exchange</li>
<li><strong>Handle errors</strong>: Separate error output from data output</li>
<li><strong>Document data flow</strong>: Comment complex pipelines</li>
<li><strong>Validate data</strong>: Check data quality at each stage</li>
<li><strong>Be efficient</strong>: Use appropriate tools for each task (rg vs grep, jq vs awk)</li>
</ol>
<h2>Next Steps</h2>
<ul>
<li>Learn advanced <code>jq</code> programming for complex data transformations</li>
<li>Explore <code>miller</code> for CSV/TSV processing</li>
<li>Study <code>awk</code> and <code>sed</code> for text processing</li>
<li>Practice building reusable pipeline components</li>
<li>Investigate distributed processing with tools like <code>parallel</code></li>
</ul>

  </body>
</html>
