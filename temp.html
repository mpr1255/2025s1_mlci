<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>streaming_data_analysis_walkthrough</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
/* Minimal sensible defaults for Notion paste */
:root { color-scheme: light; }
body { font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Arial, "Apple Color Emoji", "Segoe UI Emoji"; line-height:1.6; font-size:16px; }
h1,h2,h3,h4 { line-height:1.25; margin:1.2em 0 .5em; }
p,ul,ol,pre,figure,table,blockquote { margin: .7em 0; }
code,kbd,samp { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "DejaVu Sans Mono", Consolas, "Liberation Mono", "Courier New", monospace; }
figure.code-image { display:block; margin: .75em 0; }
figure.code-image > img { max-width: 100%; height: auto; display: block; }
blockquote { padding-left: .9em; border-left: 3px solid #e5e7eb; color:#374151; }
hr { border:0; border-top:1px solid #e5e7eb; margin:1.5em 0; }
table { border-collapse: collapse; width: 100%; }
th, td { border: 1px solid #e5e7eb; padding: .4em .6em; text-align: left; }
a { color: #2563eb; text-decoration: none; }
a:hover { text-decoration: underline; }
</style>
  </head>
  <body>
<h1>Terminal 2: Reproduce &amp; Augment a Streaming Data Analysis Pipeline</h1>
<p>This walkthrough demonstrates how command-line tools can be faster than distributed systems like Hadoop for many data processing tasks. We'll reproduce and modernize Adam Drake's famous chess analysis pipeline that was <a href="https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html"><strong>235x faster</strong> than a Hadoop cluster.</a></p>
<p>You'll learn to build streaming data pipelines using Unix tools, replace arcane <code>awk</code> with readable Python, and store results in SQLite for further analysis.</p>
<h2>Prerequisites</h2>
<h3>Install required tools</h3>
<pre><code class="language-bash"># Check what's already installed
which uv curl wget sqlite-utils tree

# Install uv if needed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install sqlite-utils
uv tool install sqlite-utils
</code></pre>
<h3>Install tree command (for directory visualization)</h3>
<p><strong>On university cluster (Nix):</strong></p>
<pre><code class="language-bash">nix profile install nixpkgs#tree
</code></pre>
<p><strong>On macOS with Homebrew:</strong></p>
<pre><code class="language-bash">brew install tree
</code></pre>
<p><strong>On Linux (Ubuntu/Debian/WSL):</strong></p>
<pre><code class="language-bash">sudo apt update &amp;&amp; sudo apt install tree
</code></pre>
<p><strong>If you don't have tree, you can see directory structure with:</strong></p>
<pre><code class="language-bash"># Alternative to tree
find . -type d | sed -e 's/[^-][^\/]*\//  /g' -e 's/^  //' -e 's/-/|/'
</code></pre>
<h2>Step 1: Set Up Proper Project Structure</h2>
<p>Learn essential file system commands and create a well-organized project:</p>
<h3>Basic File System Commands</h3>
<pre><code class="language-bash"># Create directories
mkdir my-directory              # Create single directory
mkdir -p path/to/nested/dirs   # Create nested directories (-p = parents)

# Create empty files  
touch filename.txt             # Creates empty file or updates timestamp
touch file1.txt file2.txt      # Create multiple files

# See what you created
ls                            # List files and directories
ls -la                        # List with details (long format, all files)
</code></pre>
<h3>Create Project Structure</h3>
<pre><code class="language-bash"># Create project directory
mkdir chess-analysis
cd chess-analysis

# Set up directory structure with mkdir -p
mkdir -p raw/        # Raw data - never modify these files
mkdir -p src/        # Source code and scripts  
mkdir -p out/        # Generated outputs - safe to delete anytime
mkdir -p tests/      # Test files and validation
mkdir -p ref/        # Reference materials and documentation

# Create placeholder files to see structure
touch raw/.gitkeep src/.gitkeep out/.gitkeep tests/.gitkeep ref/.gitkeep

# Verify structure with tree
tree

# Alternative if no tree command:
find . -type d | sed -e 's/[^-][^\/]*\//  /g' -e 's/^  //' -e 's/-/|/'
</code></pre>
<p><strong>Why this structure matters:</strong></p>
<ul>
<li><code>raw/</code> - Original data that you never modify</li>
<li><code>src/</code> - Your code and processing scripts</li>
<li><code>out/</code> - Generated files you can delete without worry</li>
<li>Clear separation prevents accidentally corrupting source data</li>
</ul>
<h2>Step 2: Understand the Problem and Data</h2>
<p>We're analyzing chess games in PGN (Portable Game Notation) format to count wins/losses/draws.</p>
<p><strong>Sample PGN format:</strong></p>
<pre><code>[Event &quot;F/S Return Match&quot;]
[Site &quot;Belgrade, Serbia Yugoslavia|JUG&quot;] 
[Date &quot;1992.11.04&quot;]
[Round &quot;29&quot;]
[White &quot;Fischer, Robert J.&quot;]
[Black &quot;Spassky, Boris V.&quot;]
[Result &quot;1-0&quot;]
(actual chess moves follow...)
</code></pre>
<p><strong>What we care about:</strong></p>
<ul>
<li><code>[Result &quot;1-0&quot;]</code> = White wins</li>
<li><code>[Result &quot;0-1&quot;]</code> = Black wins</li>
<li><code>[Result &quot;1/2-1/2&quot;]</code> = Draw</li>
<li><code>[Result &quot;*&quot;]</code> = Ongoing/unknown (ignore)</li>
</ul>
<h2>Step 3: Download Chess Data</h2>
<p>Let's get some real chess data and learn to examine files:</p>
<pre><code class="language-bash"># Move to raw data directory
cd raw/

# Download some sample chess data (these are example URLs - you may need to find current ones)
curl -o sample.pgn &quot;https://www.chess.com/games/download/pgn&quot;

# For this demo, let's create a sample PGN file to work with
cat &gt; sample.pgn &lt;&lt; 'EOF'
[Event &quot;Sample Tournament&quot;]
[Site &quot;Online&quot;]
[Date &quot;2024.01.15&quot;] 
[Round &quot;1&quot;]
[White &quot;Player A&quot;]
[Black &quot;Player B&quot;]
[Result &quot;1-0&quot;]

1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 1-0

[Event &quot;Another Game&quot;]
[Site &quot;Club Match&quot;]
[Date &quot;2024.01.16&quot;]
[Round &quot;2&quot;] 
[White &quot;Player C&quot;]
[Black &quot;Player D&quot;]
[Result &quot;0-1&quot;]

1. d4 d5 2. c4 e6 3. Nc3 0-1

[Event &quot;Draw Example&quot;]
[Site &quot;Tournament&quot;]
[Date &quot;2024.01.17&quot;]
[Round &quot;3&quot;]
[White &quot;Player E&quot;] 
[Black &quot;Player F&quot;]
[Result &quot;1/2-1/2&quot;]

1. e4 c5 2. Nf3 d6 1/2-1/2
EOF

# Check what we have
ls -lh
</code></pre>
<h3>Learn File Examination Commands</h3>
<p>Now let's learn essential commands for examining our data:</p>
<pre><code class="language-bash"># Count lines in files
wc -l sample.pgn              # Count lines 
wc -l *.pgn                   # Count lines in all PGN files
wc -w sample.pgn              # Count words
wc -c sample.pgn              # Count characters

# Look at file contents
cat sample.pgn                # Show entire file content
head sample.pgn               # Show first 10 lines
head -5 sample.pgn            # Show first 5 lines
tail sample.pgn               # Show last 10 lines  
tail -5 sample.pgn            # Show last 5 lines

# Look at specific parts
grep &quot;Result&quot; sample.pgn      # Find lines containing &quot;Result&quot;
grep -n &quot;Event&quot; sample.pgn    # Show line numbers (-n flag)
grep -i &quot;WHITE&quot; sample.pgn    # Case-insensitive search (-i flag)
</code></pre>
<h3>Understanding PGN Structure</h3>
<pre><code class="language-bash"># Let's examine the structure of our chess data
echo &quot;=== First few lines ===&quot;
head -10 sample.pgn

echo -e &quot;\n=== All Result lines ===&quot;
grep &quot;Result&quot; sample.pgn

echo -e &quot;\n=== Count different result types ===&quot;
grep &quot;Result&quot; sample.pgn | sort | uniq -c

echo -e &quot;\n=== File statistics ===&quot;
echo &quot;Total lines: $(wc -l &lt; sample.pgn)&quot;
echo &quot;Result lines: $(grep -c &quot;Result&quot; sample.pgn)&quot;
echo &quot;Games: $(grep -c &quot;Event&quot; sample.pgn)&quot;
</code></pre>
<h2>Step 4: Build the Basic Pipeline</h2>
<p>Let's start with Adam Drake's approach, then improve it.</p>
<h3>Original approach (simple but slow):</h3>
<pre><code class="language-bash">cd ../  # Back to project root

# Simple pipeline - counts all results
cat raw/*.pgn | grep &quot;Result&quot; | sort | uniq -c
</code></pre>
<h3>Faster approach with awk (but hard to read):</h3>
<pre><code class="language-bash"># Adam Drake's optimized version - but awk is cryptic!
cat raw/*.pgn | grep &quot;Result&quot; | awk '{ split($0, a, &quot;-&quot;); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
</code></pre>
<h2>Step 5: Replace AWK with Readable Python</h2>
<p>Let's create a Python script that's much easier to understand and maintain:</p>
<pre><code class="language-bash"># Create our Python parser
cat &gt; src/parse_chess_results.py &lt;&lt; 'EOF'
#!/usr/bin/env uv run
# /// script
# requires-python = &quot;&gt;=3.9&quot;
# dependencies = [
#     &quot;typer&quot;,
# ]
# ///

&quot;&quot;&quot;
Modern replacement for the arcane awk chess parser.
Reads chess game results from stdin and outputs counts.
&quot;&quot;&quot;

import sys
import re
import json
import typer

app = typer.Typer(help=&quot;Parse chess game results from PGN files&quot;)

def parse_result_line(line: str) -&gt; str | None:
    &quot;&quot;&quot;Extract result from a PGN Result line.&quot;&quot;&quot;
    # Look for [Result &quot;1-0&quot;] style lines
    match = re.search(r'\[Result\s+&quot;([^&quot;]+)&quot;\]', line)
    if not match:
        return None
    
    result = match.group(1)
    
    # Map results to our categories
    if result == &quot;1-0&quot;:
        return &quot;white_wins&quot;
    elif result == &quot;0-1&quot;: 
        return &quot;black_wins&quot;
    elif result == &quot;1/2-1/2&quot;:
        return &quot;draws&quot;
    else:
        return &quot;other&quot;

@app.command()
def count_results(
    json_output: bool = typer.Option(False, &quot;--json&quot;, help=&quot;Output as JSON&quot;)
):
    &quot;&quot;&quot;Count chess game results from stdin.&quot;&quot;&quot;
    
    counts = {
        &quot;white_wins&quot;: 0,
        &quot;black_wins&quot;: 0,
        &quot;draws&quot;: 0,
        &quot;other&quot;: 0
    }
    
    total_lines = 0
    for line in sys.stdin:
        total_lines += 1
        
        # Only process Result lines
        if &quot;Result&quot; not in line:
            continue
            
        result = parse_result_line(line.strip())
        if result:
            counts[result] += 1
    
    # Output results
    total_games = counts[&quot;white_wins&quot;] + counts[&quot;black_wins&quot;] + counts[&quot;draws&quot;]
    
    if json_output:
        output = {
            **counts,
            &quot;total_games&quot;: total_games,
            &quot;total_lines_processed&quot;: total_lines
        }
        print(json.dumps(output))
    else:
        print(f&quot;Total games: {total_games}&quot;)
        print(f&quot;White wins: {counts['white_wins']}&quot;)
        print(f&quot;Black wins: {counts['black_wins']}&quot;)
        print(f&quot;Draws: {counts['draws']}&quot;)
        print(f&quot;Other: {counts['other']}&quot;)

if __name__ == &quot;__main__&quot;:
    app()
EOF

# Make it executable
chmod +x src/parse_chess_results.py
</code></pre>
<h2>Step 6: Test the Python Pipeline</h2>
<pre><code class="language-bash"># Test with a single file first
cat raw/sample.pgn | grep &quot;Result&quot; | ./src/parse_chess_results.py

# Test with JSON output
cat raw/sample.pgn | grep &quot;Result&quot; | ./src/parse_chess_results.py --json

# Full pipeline with all files
cat raw/*.pgn | grep &quot;Result&quot; | ./src/parse_chess_results.py
</code></pre>
<p>Compare this to the cryptic awk version - much more readable and maintainable!</p>
<h2>Step 7: Optimize with Parallel Processing</h2>
<p>Let's reproduce Adam Drake's parallel optimization, but with our Python script:</p>
<pre><code class="language-bash"># Sequential processing (baseline)
time (cat raw/*.pgn | grep &quot;Result&quot; | ./src/parse_chess_results.py)

# Parallel processing with xargs
time (find raw/ -name &quot;*.pgn&quot; -print0 | \
      xargs -0 -n1 -P4 grep &quot;Result&quot; | \
      ./src/parse_chess_results.py)
</code></pre>
<p><strong>What's happening:</strong></p>
<ul>
<li><code>find raw/ -name &quot;*.pgn&quot; -print0</code> - Find all PGN files, null-terminated</li>
<li><code>xargs -0 -n1 -P4</code> - Run grep on each file in parallel (4 processes)</li>
<li>Final Python script aggregates all results</li>
</ul>
<h2>Step 8: Store Results in SQLite Database</h2>
<p>Now let's enhance our pipeline to store results in a database:</p>
<pre><code class="language-bash"># Create enhanced version that outputs structured data
cat &gt; src/parse_chess_to_db.py &lt;&lt; 'EOF'
#!/usr/bin/env uv run
# /// script
# requires-python = &quot;&gt;=3.9&quot;
# dependencies = [
#     &quot;typer&quot;,
# ]
# ///

&quot;&quot;&quot;
Parse chess results and output JSONL for database insertion.
&quot;&quot;&quot;

import sys
import re
import json
from datetime import datetime
import typer

def parse_game_metadata(content: str) -&gt; dict:
    &quot;&quot;&quot;Extract game metadata from PGN content.&quot;&quot;&quot;
    metadata = {}
    
    # Extract common PGN tags
    patterns = {
        'event': r'\[Event\s+&quot;([^&quot;]+)&quot;\]',
        'site': r'\[Site\s+&quot;([^&quot;]+)&quot;\]', 
        'date': r'\[Date\s+&quot;([^&quot;]+)&quot;\]',
        'white': r'\[White\s+&quot;([^&quot;]+)&quot;\]',
        'black': r'\[Black\s+&quot;([^&quot;]+)&quot;\]',
        'result': r'\[Result\s+&quot;([^&quot;]+)&quot;\]'
    }
    
    for key, pattern in patterns.items():
        match = re.search(pattern, content)
        metadata[key] = match.group(1) if match else None
    
    return metadata

@app.command()
def main():
    &quot;&quot;&quot;Parse chess games and output JSONL records.&quot;&quot;&quot;
    
    game_buffer = []
    current_game = &quot;&quot;
    
    for line in sys.stdin:
        current_game += line
        
        # End of game indicated by empty line after moves
        if line.strip() == &quot;&quot; and current_game.strip():
            if &quot;[Result&quot; in current_game:
                metadata = parse_game_metadata(current_game)
                
                # Add processing timestamp
                metadata['processed_at'] = datetime.now().isoformat()
                
                # Categorize result
                result = metadata.get('result', '*')
                if result == &quot;1-0&quot;:
                    metadata['result_category'] = &quot;white_wins&quot;
                elif result == &quot;0-1&quot;:
                    metadata['result_category'] = &quot;black_wins&quot; 
                elif result == &quot;1/2-1/2&quot;:
                    metadata['result_category'] = &quot;draws&quot;
                else:
                    metadata['result_category'] = &quot;other&quot;
                
                # Output as JSONL
                print(json.dumps(metadata))
            
            current_game = &quot;&quot;

app = typer.Typer()
app.command()(main)

if __name__ == &quot;__main__&quot;:
    app()
EOF

chmod +x src/parse_chess_to_db.py
</code></pre>
<h2>Step 9: Build Complete Data Pipeline</h2>
<p>Now let's create the full pipeline that processes chess files and stores in SQLite:</p>
<pre><code class="language-bash"># Process files and store in database
find raw/ -name &quot;*.pgn&quot; -print0 | \
    xargs -0 -n1 -P4 cat | \
    ./src/parse_chess_to_db.py | \
    sqlite-utils insert out/chess_games.db games - --pk=event --pk=white --pk=black --pk=date

# Query the results
sqlite-utils query out/chess_games.db &quot;
    SELECT 
        result_category,
        COUNT(*) as game_count,
        COUNT(*) * 100.0 / (SELECT COUNT(*) FROM games) as percentage
    FROM games 
    GROUP BY result_category
&quot;

# More detailed analysis
sqlite-utils query out/chess_games.db &quot;
    SELECT 
        substr(date, 1, 4) as year,
        result_category,
        COUNT(*) as games
    FROM games 
    WHERE date IS NOT NULL 
    GROUP BY year, result_category
    ORDER BY year, result_category
&quot; --table
</code></pre>
<h2>Step 10: Performance Comparison and Benchmarking</h2>
<p>Let's measure our pipeline performance:</p>
<pre><code class="language-bash"># Create benchmark script
cat &gt; src/benchmark.py &lt;&lt; 'EOF' 
#!/usr/bin/env uv run
# /// script
# requires-python = &quot;&gt;=3.9&quot;
# dependencies = [
#     &quot;typer&quot;,
# ]
# ///

import subprocess
import time
import typer

def run_pipeline(description: str, command: str):
    &quot;&quot;&quot;Time a pipeline command.&quot;&quot;&quot;
    print(f&quot;\n🏃 {description}&quot;)
    print(f&quot;Command: {command}&quot;)
    
    start = time.time()
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    end = time.time()
    
    duration = end - start
    print(f&quot;⏱️  Duration: {duration:.2f} seconds&quot;)
    
    if result.returncode == 0:
        print(&quot;✅ Success&quot;)
        if result.stdout.strip():
            print(f&quot;Output: {result.stdout.strip()}&quot;)
    else:
        print(&quot;❌ Failed&quot;)
        print(f&quot;Error: {result.stderr}&quot;)
    
    return duration

@typer.command()
def main():
    &quot;&quot;&quot;Benchmark different pipeline approaches.&quot;&quot;&quot;
    
    # Test with available data
    pipelines = [
        (&quot;Simple grep + Python&quot;, 
         &quot;cat raw/*.pgn | grep 'Result' | ./src/parse_chess_results.py&quot;),
        
        (&quot;Parallel grep + Python&quot;,
         &quot;find raw/ -name '*.pgn' -print0 | xargs -0 -n1 -P4 grep 'Result' | ./src/parse_chess_results.py&quot;),
        
        (&quot;Full parallel pipeline to database&quot;,
         &quot;find raw/ -name '*.pgn' -print0 | xargs -0 -n1 -P4 cat | ./src/parse_chess_to_db.py | wc -l&quot;)
    ]
    
    print(&quot;🎯 Chess Pipeline Benchmarks&quot;)
    print(&quot;=&quot; * 50)
    
    results = []
    for desc, cmd in pipelines:
        duration = run_pipeline(desc, cmd)
        results.append((desc, duration))
    
    print(&quot;\n📊 Summary:&quot;)
    print(&quot;-&quot; * 30)
    for desc, duration in results:
        print(f&quot;{desc}: {duration:.2f}s&quot;)
    
    # Calculate speedup
    if len(results) &gt; 1:
        baseline = results[0][1]
        for desc, duration in results[1:]:
            speedup = baseline / duration if duration &gt; 0 else 0
            print(f&quot;   → {speedup:.1f}x faster than baseline&quot;)

if __name__ == &quot;__main__&quot;:
    typer.run(main)
EOF

chmod +x src/benchmark.py

# Run benchmarks
./src/benchmark.py
</code></pre>
<h2>Step 11: Advanced Analysis Queries</h2>
<p>Now that we have structured data, we can do sophisticated analysis:</p>
<pre><code class="language-bash"># Top events by game count
sqlite-utils query out/chess_games.db &quot;
    SELECT event, COUNT(*) as games 
    FROM games 
    GROUP BY event 
    ORDER BY games DESC 
    LIMIT 10
&quot; --table

# Win rates by player (for players with many games)
sqlite-utils query out/chess_games.db &quot;
    WITH player_stats AS (
        SELECT white as player, 
               SUM(CASE WHEN result_category = 'white_wins' THEN 1 ELSE 0 END) as wins,
               COUNT(*) as total_games
        FROM games 
        GROUP BY white
        HAVING total_games &gt;= 10
    )
    SELECT player, wins, total_games, 
           ROUND(wins * 100.0 / total_games, 2) as win_percentage
    FROM player_stats
    ORDER BY win_percentage DESC
    LIMIT 10
&quot; --table

# Temporal patterns - draws over time
sqlite-utils query out/chess_games.db &quot;
    SELECT substr(date, 1, 4) as year,
           AVG(CASE WHEN result_category = 'draws' THEN 1.0 ELSE 0.0 END) as draw_rate
    FROM games 
    WHERE date IS NOT NULL AND date != '????.??.??'
    GROUP BY year
    ORDER BY year
&quot; --table
</code></pre>
<h2>Step 12: Create Reusable Pipeline Scripts</h2>
<p>Let's package this into reusable scripts:</p>
<pre><code class="language-bash"># Create main pipeline runner
cat &gt; src/run_analysis.sh &lt;&lt; 'EOF'
#!/bin/bash
# Complete chess analysis pipeline

set -e  # Exit on error

echo &quot;🏁 Starting chess analysis pipeline...&quot;

# Check dependencies
echo &quot;📋 Checking dependencies...&quot;
command -v sqlite-utils &gt;/dev/null || { echo &quot;❌ sqlite-utils not found&quot;; exit 1; }

# Create output directory
mkdir -p out/

# Run analysis
echo &quot;⚡ Processing chess files...&quot;
find raw/ -name &quot;*.pgn&quot; -print0 | \
    xargs -0 -n1 -P$(nproc) cat | \
    ./src/parse_chess_to_db.py | \
    sqlite-utils insert out/chess_games.db games - --replace

echo &quot;📊 Analysis complete!&quot;
echo &quot;Database: out/chess_games.db&quot;

# Show summary
echo -e &quot;\n📈 Summary:&quot;
sqlite-utils query out/chess_games.db &quot;
    SELECT 
        result_category,
        COUNT(*) as games,
        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM games), 1) as percentage
    FROM games 
    GROUP BY result_category
    ORDER BY games DESC
&quot; --table

echo -e &quot;\n✅ Pipeline complete! Query the database with:&quot;
echo &quot;   sqlite-utils query out/chess_games.db 'YOUR_SQL_HERE'&quot;
EOF

chmod +x src/run_analysis.sh

# Run the complete pipeline
./src/run_analysis.sh
</code></pre>
<h2>Key Learning Points</h2>
<ul>
<li><strong>Stream processing beats batch processing</strong> for many tasks - no need to load everything into memory</li>
<li><strong>Parallel pipelines</strong> can dramatically improve performance using <code>xargs -P</code></li>
<li><strong>Replace arcane tools</strong> (like complex awk) with readable Python while keeping performance</li>
<li><strong>Proper directory structure</strong> keeps raw data safe and outputs organized</li>
<li><strong>SQLite is powerful</strong> for analysis - much better than just counting with scripts</li>
<li><strong>Unix philosophy</strong> - small composable tools that work together</li>
</ul>
<h2>Performance Insights</h2>
<p>Adam Drake's original findings:</p>
<ul>
<li><strong>Hadoop cluster (7 machines)</strong>: 26 minutes for 1.75GB = ~1.14MB/sec</li>
<li><strong>Optimized shell pipeline</strong>: 12 seconds for 3.46GB = ~270MB/sec</li>
<li><strong>Result</strong>: 235x faster than Hadoop!</li>
</ul>
<p>Our modernized version maintains the performance benefits while being much more:</p>
<ul>
<li><strong>Readable</strong> - Python instead of cryptic awk</li>
<li><strong>Maintainable</strong> - proper structure and error handling</li>
<li><strong>Extensible</strong> - SQLite database enables complex analysis</li>
<li><strong>Reproducible</strong> - clear workflow and dependencies</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Add data validation and error handling</li>
<li>Implement incremental processing for new files</li>
<li>Create visualization dashboards using the SQLite data</li>
<li>Experiment with other parallel processing tools</li>
<li>Scale up to even larger datasets</li>
<li>Compare with modern big data tools (Spark, DuckDB, etc.)</li>
</ul>

  </body>
</html>
