<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>mensa_scraping_walkthrough</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
/* Minimal sensible defaults for Notion paste */
:root { color-scheme: light; }
body { font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Arial, "Apple Color Emoji", "Segoe UI Emoji"; line-height:1.6; font-size:16px; }
h1,h2,h3,h4 { line-height:1.25; margin:1.2em 0 .5em; }
p,ul,ol,pre,figure,table,blockquote { margin: .7em 0; }
code,kbd,samp { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "DejaVu Sans Mono", Consolas, "Liberation Mono", "Courier New", monospace; }
figure.code-image { display:block; margin: .75em 0; }
figure.code-image > img { max-width: 100%; height: auto; display: block; }
blockquote { padding-left: .9em; border-left: 3px solid #e5e7eb; color:#374151; }
hr { border:0; border-top:1px solid #e5e7eb; margin:1.5em 0; }
table { border-collapse: collapse; width: 100%; }
th, td { border: 1px solid #e5e7eb; padding: .4em .6em; text-align: left; }
a { color: #2563eb; text-decoration: none; }
a:hover { text-decoration: underline; }
</style>
  </head>
  <body>
<h1>Mensa Menu Scraping Walkthrough</h1>
<p>This walkthrough teaches you how to scrape data from web APIs by reverse engineering network requests. You'll learn to inspect browser network traffic, extract API calls, and build your own scraping script.</p>
<h2>Prerequisites</h2>
<p>Make sure you have <code>uv</code> installed:</p>
<pre><code class="language-bash">which uv
</code></pre>
<p>If not installed, follow the installation instructions at https://docs.astral.sh/uv/getting-started/installation/</p>
<h2>Step 1: Inspect the Target Website</h2>
<p>Navigate to the Mensa menu page:</p>
<p>https://www.stw-ma.de/en/food-drink/menus/mensa-am-schloss-en/</p>
<ol>
<li>Open your browser's Developer Tools (F12 or right-click â†’ &quot;Inspect&quot;)</li>
<li>Go to the <strong>Network</strong> tab</li>
<li>Reload the page to capture all network requests</li>
<li>Look for requests that might contain the menu data (often JSON or API calls)</li>
</ol>
<h2>Step 2: Find the API Endpoint</h2>
<p>In the Network tab, look for a request that returns the menu data. You should find something like:</p>
<ul>
<li>URL: <code>https://api.stw-ma.de/tl1/menuplan</code></li>
<li>Method: POST</li>
<li>Contains form data with parameters like date, location, etc.</li>
</ul>
<p>Right-click on this request and select &quot;Copy as cURL&quot; to get the full command.</p>
<h2>Step 3: Test the Raw cURL Command</h2>
<p>The copied cURL command will look something like this (with many headers):</p>
<pre><code class="language-bash">curl 'https://api.stw-ma.de/tl1/menuplan' \
  -H 'Accept: */*' \
  -H 'Accept-Language: en-US,en;q=0.9' \
  -H 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \
  -H 'Cookie: [long cookie string]' \
  -H 'Origin: https://www.stw-ma.de' \
  -H 'Referer: https://www.stw-ma.de/en/food-drink/menus/mensa-am-schloss-en/' \
  -H 'User-Agent: [long user agent string]' \
  --data-raw 'id=bc003e93a1e942f45a99dcf8082da289&amp;location=610&amp;lang=en&amp;date=2025-08-27&amp;mode=day'
</code></pre>
<p>Test this command in your terminal to verify it works.</p>
<h2>Step 4: Simplify the cURL Command</h2>
<p>Most headers are unnecessary. Reduce it to the essentials:</p>
<pre><code class="language-bash">curl 'https://api.stw-ma.de/tl1/menuplan' -s \
  -H 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \
  --data-raw &quot;id=bc003e93a1e942f45a99dcf8082da289&amp;location=610&amp;lang=en&amp;date=2025-08-27&amp;mode=day&quot;
</code></pre>
<p>The <code>-s</code> flag makes curl silent (no progress bar).</p>
<h2>Step 5: Examine the Response</h2>
<p>Add <code>| jq .</code> to format the JSON response nicely:</p>
<pre><code class="language-bash">curl 'https://api.stw-ma.de/tl1/menuplan' -s \
  -H 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \
  --data-raw &quot;id=bc003e93a1e942f45a99dcf8082da289&amp;location=610&amp;lang=en&amp;date=2025-08-27&amp;mode=day&quot; \
  | jq .
</code></pre>
<p>You'll see the response contains HTML in a <code>content</code> field.</p>
<h2>Step 6: Extract and Parse the HTML</h2>
<p>Extract just the HTML content and parse the table:</p>
<pre><code class="language-bash">curl 'https://api.stw-ma.de/tl1/menuplan' -s \
  -H 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \
  --data-raw &quot;id=bc003e93a1e942f45a99dcf8082da289&amp;location=610&amp;lang=en&amp;date=2025-08-27&amp;mode=day&quot; \
  | jq -r .content \
  | pup 'table.speiseplan-table tr json{}'
</code></pre>
<p>This uses <code>pup</code> (a command-line HTML parser) to extract table rows as JSON.</p>
<h2>Step 7: Download the Python Script</h2>
<p>Instead of manually parsing, we'll use a pre-built Python script. Download it:</p>
<pre><code class="language-bash">curl -o scrape.py https://raw.githubusercontent.com/mpr1255/2025s1_mlci/master/week2--scraping_api_cli/mensa_scraping/first_example/scrape.py
</code></pre>
<h2>Step 8: Understand the Python Script</h2>
<p>The script (<code>scrape.py</code>) does several important things:</p>
<ol>
<li><strong>Shebang line</strong>: <code>#!/usr/bin/env -S uv run</code> - This tells the system to run the script with <code>uv</code></li>
<li><strong>Inline dependencies</strong>: The script declares its Python dependencies inline using PEP 723 format</li>
<li><strong>CLI interface</strong>: Uses <code>typer</code> to create a command-line interface</li>
<li><strong>Configurable payload</strong>: The API parameters are stored in a dictionary that you can modify</li>
</ol>
<p>Key parts of the payload you can modify:</p>
<ul>
<li><code>date</code>: Change to any date in YYYY-MM-DD format</li>
<li><code>location</code>: Different mensa locations have different IDs</li>
<li><code>lang</code>: Switch between &quot;en&quot; and &quot;de&quot;</li>
</ul>
<h2>Step 9: Run the Script</h2>
<p>Test the script with today's date:</p>
<pre><code class="language-bash">python scrape.py 2025-08-27
</code></pre>
<p>Try different output formats:</p>
<pre><code class="language-bash"># CSV output (default)
python scrape.py 2025-08-27

# JSON output
python scrape.py 2025-08-27 --json

# Explicit CSV
python scrape.py 2025-08-27 --csv
</code></pre>
<h2>Step 10: Build Data Processing Pipelines</h2>
<p>You can pipe the output to other tools for further processing:</p>
<pre><code class="language-bash"># Save to SQLite database
python scrape.py 2025-08-27 | sqlite-utils insert menu.db menu - --csv --pk=date --pk=category

# Process multiple dates
for date in 2025-08-26 2025-08-27 2025-08-28; do
  python scrape.py $date | sqlite-utils insert menu.db menu - --csv --pk=date --pk=category
done

# Query the database
sqlite-utils query menu.db &quot;SELECT * FROM menu WHERE category LIKE '%Soup%'&quot;
</code></pre>
<h2>Step 11: Experiment and Modify</h2>
<p>Try modifying the script:</p>
<ol>
<li>Change the <code>location</code> parameter to scrape different mensa locations</li>
<li>Modify the parsing logic to extract additional information</li>
<li>Add error handling for missing data</li>
<li>Create a script that automatically fetches multiple days</li>
</ol>
<h2>Key Learning Points</h2>
<ul>
<li><strong>Network inspection</strong>: Browser dev tools reveal how web applications communicate with servers</li>
<li><strong>API reverse engineering</strong>: You can extract API calls from browser traffic</li>
<li><strong>cURL simplification</strong>: Most headers are optional; focus on the essential ones</li>
<li><strong>Shebang scripts</strong>: Modern Python scripts can declare dependencies inline</li>
<li><strong>CLI design</strong>: Tools like <code>typer</code> make it easy to create professional command-line interfaces</li>
<li><strong>Data pipelines</strong>: Unix philosophy of small, composable tools working together</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Explore other APIs using similar techniques</li>
<li>Learn about rate limiting and ethical scraping practices</li>
<li>Build more sophisticated data processing pipelines</li>
<li>Investigate authentication and session handling for protected APIs</li>
</ul>

  </body>
</html>
